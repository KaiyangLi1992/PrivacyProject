{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from mincut_pool_net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "max_nodes = 111\n",
    "torch.manual_seed(12315)\n",
    "\n",
    "dataset = TUDataset(root='./tmp', name='NCI1', transform=T.ToDense(max_nodes))\n",
    "dataset = dataset.shuffle()\n",
    "dataset_length = len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# split dataset into 3 parts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "D_target = dataset[:int(0.4 * dataset_length)]\n",
    "D_aux = dataset[int(0.4 * dataset_length): int(0.7 * dataset_length)]\n",
    "G_target = dataset[int(0.7 * dataset_length):]\n",
    "\n",
    "D_target_train = D_target[:int(0.6 * len(D_target))]\n",
    "D_target_test = D_target[int(0.6 * len(D_target)):]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TrainSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item].x, self.data[item].adj, self.data[item].y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_dataset = TrainSet(D_target_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "test_dataset = TrainSet(D_target_test)\n",
    "test_loader = DataLoader(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        x = data[0].to(device)\n",
    "        adj = data[1].to(device)\n",
    "        y = data[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, mc_loss, o_loss, _ = model(x, adj)\n",
    "        loss = F.nll_loss(out, y.view(-1)) + mc_loss + o_loss\n",
    "        loss.backward()\n",
    "        loss_all += y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_all / len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        x = data[0].to(device)\n",
    "        adj = data[1].to(device)\n",
    "        y = data[2].to(device)\n",
    "        pred, mc_loss, o_loss, _ = model(x, adj)\n",
    "        loss = F.nll_loss(pred, y.view(-1)) + mc_loss + o_loss\n",
    "        loss_all += y.size(0) * float(loss)\n",
    "        correct += int(pred.max(dim=1)[1].eq(y.view(-1)).sum())\n",
    "\n",
    "    return loss_all / len(loader.dataset), correct / len(loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Train Loss: 1.2840, Test Loss: 1.2812, Best Train Accu: 0.6004, Best Test Accu: 0.5897\n",
      "Epoch: 0002, Train Loss: 1.2794, Test Loss: 1.2800, Best Train Accu: 0.6004, Best Test Accu: 0.5897\n",
      "Epoch: 0003, Train Loss: 1.2769, Test Loss: 1.2781, Best Train Accu: 0.6004, Best Test Accu: 0.5897\n",
      "Epoch: 0004, Train Loss: 1.2746, Test Loss: 1.2759, Best Train Accu: 0.6004, Best Test Accu: 0.5897\n",
      "Epoch: 0005, Train Loss: 1.2719, Test Loss: 1.2726, Best Train Accu: 0.6004, Best Test Accu: 0.5897\n",
      "Epoch: 0006, Train Loss: 1.2695, Test Loss: 1.2689, Best Train Accu: 0.6197, Best Test Accu: 0.6049\n",
      "Epoch: 0007, Train Loss: 1.2668, Test Loss: 1.2660, Best Train Accu: 0.6197, Best Test Accu: 0.6109\n",
      "Epoch: 0008, Train Loss: 1.2644, Test Loss: 1.2637, Best Train Accu: 0.6197, Best Test Accu: 0.6109\n",
      "Epoch: 0009, Train Loss: 1.2620, Test Loss: 1.2612, Best Train Accu: 0.6217, Best Test Accu: 0.6155\n",
      "Epoch: 0010, Train Loss: 1.2595, Test Loss: 1.2583, Best Train Accu: 0.6217, Best Test Accu: 0.6155\n",
      "Epoch: 0011, Train Loss: 1.2571, Test Loss: 1.2550, Best Train Accu: 0.6217, Best Test Accu: 0.6155\n",
      "Epoch: 0012, Train Loss: 1.2547, Test Loss: 1.2517, Best Train Accu: 0.6217, Best Test Accu: 0.6155\n",
      "Epoch: 0013, Train Loss: 1.2522, Test Loss: 1.2498, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0014, Train Loss: 1.2500, Test Loss: 1.2472, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0015, Train Loss: 1.2474, Test Loss: 1.2435, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0016, Train Loss: 1.2456, Test Loss: 1.2420, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0017, Train Loss: 1.2441, Test Loss: 1.2409, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0018, Train Loss: 1.2426, Test Loss: 1.2381, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0019, Train Loss: 1.2417, Test Loss: 1.2383, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0020, Train Loss: 1.2406, Test Loss: 1.2364, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0021, Train Loss: 1.2400, Test Loss: 1.2341, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0022, Train Loss: 1.2400, Test Loss: 1.2359, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0023, Train Loss: 1.2387, Test Loss: 1.2335, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0024, Train Loss: 1.2388, Test Loss: 1.2322, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0025, Train Loss: 1.2381, Test Loss: 1.2343, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0026, Train Loss: 1.2384, Test Loss: 1.2322, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0027, Train Loss: 1.2378, Test Loss: 1.2337, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0028, Train Loss: 1.2373, Test Loss: 1.2322, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0029, Train Loss: 1.2374, Test Loss: 1.2328, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0030, Train Loss: 1.2372, Test Loss: 1.2330, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0031, Train Loss: 1.2376, Test Loss: 1.2309, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0032, Train Loss: 1.2367, Test Loss: 1.2315, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0033, Train Loss: 1.2367, Test Loss: 1.2311, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0034, Train Loss: 1.2366, Test Loss: 1.2326, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0035, Train Loss: 1.2369, Test Loss: 1.2298, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0036, Train Loss: 1.2360, Test Loss: 1.2313, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0037, Train Loss: 1.2358, Test Loss: 1.2327, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0038, Train Loss: 1.2362, Test Loss: 1.2307, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0039, Train Loss: 1.2357, Test Loss: 1.2312, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0040, Train Loss: 1.2356, Test Loss: 1.2310, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0041, Train Loss: 1.2354, Test Loss: 1.2311, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0042, Train Loss: 1.2353, Test Loss: 1.2301, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0043, Train Loss: 1.2352, Test Loss: 1.2301, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0044, Train Loss: 1.2352, Test Loss: 1.2310, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0045, Train Loss: 1.2362, Test Loss: 1.2293, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0046, Train Loss: 1.2349, Test Loss: 1.2307, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0047, Train Loss: 1.2355, Test Loss: 1.2332, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0048, Train Loss: 1.2350, Test Loss: 1.2298, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0049, Train Loss: 1.2348, Test Loss: 1.2289, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0050, Train Loss: 1.2347, Test Loss: 1.2292, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0051, Train Loss: 1.2345, Test Loss: 1.2293, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0052, Train Loss: 1.2344, Test Loss: 1.2294, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0053, Train Loss: 1.2348, Test Loss: 1.2296, Best Train Accu: 0.6217, Best Test Accu: 0.6170\n",
      "Epoch: 0054, Train Loss: 1.2358, Test Loss: 1.2344, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0055, Train Loss: 1.2347, Test Loss: 1.2282, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0056, Train Loss: 1.2344, Test Loss: 1.2277, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0057, Train Loss: 1.2339, Test Loss: 1.2294, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0058, Train Loss: 1.2341, Test Loss: 1.2330, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0059, Train Loss: 1.2342, Test Loss: 1.2305, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0060, Train Loss: 1.2343, Test Loss: 1.2267, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0061, Train Loss: 1.2341, Test Loss: 1.2285, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0062, Train Loss: 1.2343, Test Loss: 1.2330, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0063, Train Loss: 1.2337, Test Loss: 1.2298, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0064, Train Loss: 1.2338, Test Loss: 1.2267, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0065, Train Loss: 1.2336, Test Loss: 1.2283, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0066, Train Loss: 1.2331, Test Loss: 1.2299, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0067, Train Loss: 1.2338, Test Loss: 1.2321, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0068, Train Loss: 1.2330, Test Loss: 1.2280, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0069, Train Loss: 1.2331, Test Loss: 1.2273, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n",
      "Epoch: 0070, Train Loss: 1.2333, Test Loss: 1.2277, Best Train Accu: 0.6268, Best Test Accu: 0.6170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19196\\837600604.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mtrain_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_accu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mtest_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_accu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtrain_accu\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mbest_train_accu\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[0mbest_train_accu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_accu\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Softwares\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19196\\467851045.py\u001B[0m in \u001B[0;36mtest\u001B[1;34m(loader)\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[0madj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m         \u001B[0mpred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmc_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mo_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnll_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmc_loss\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mo_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mloss_all\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Softwares\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Github\\Inference-Attach\\mincut_pool_net.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x, adj)\u001B[0m\n\u001B[0;32m     33\u001B[0m         \u001B[1;31m# print(s.shape)    # 200 x 111 x 56\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m         \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmc1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mo1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdense_mincut_pool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m         \u001B[1;31m# print(x.shape)  # 200 x 56 x 192\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m         \u001B[1;31m# print(adj.shape)    # 200 x 56 x 56\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Softwares\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch_geometric\\nn\\dense\\mincut_pool.py\u001B[0m in \u001B[0;36mdense_mincut_pool\u001B[1;34m(x, adj, s, mask)\u001B[0m\n\u001B[0;32m     72\u001B[0m     \u001B[0mmincut_num\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_rank3_trace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_adj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m     \u001B[0md_flat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'ijk->ij'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m     \u001B[0md\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_rank3_diag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0md_flat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m     mincut_den = _rank3_trace(\n\u001B[0;32m     76\u001B[0m         torch.matmul(torch.matmul(s.transpose(1, 2), d), s))\n",
      "\u001B[1;32mD:\\Softwares\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch_geometric\\nn\\dense\\mincut_pool.py\u001B[0m in \u001B[0;36m_rank3_diag\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_rank3_diag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 103\u001B[1;33m     \u001B[0meye\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meye\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_as\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    104\u001B[0m     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0meye\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = test_loss = float('inf')\n",
    "best_train_accu = best_test_accu = 0\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    train_loss = train()\n",
    "    _, train_accu = test(train_loader)\n",
    "    test_loss, test_accu = test(test_loader)\n",
    "    if train_accu > best_train_accu:\n",
    "        best_train_accu = train_accu\n",
    "    if test_accu > best_test_accu:\n",
    "        best_test_accu = test_accu\n",
    "    print(\n",
    "        f'Epoch: {epoch:04d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Accu: {train_accu:.4f}, Test Accu: {test_accu:.4f}, Best Train Accu: {best_train_accu:.4f}, Best Test Accu: {best_test_accu:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_save_path = \"NCI_model_mincut_pool.pt\"\n",
    "torch.save(model, data_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
